{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latency and memory\n",
    "\n",
    "## How to run this file on the lisa cluster\n",
    "\n",
    "Start by pulling the lastest version of the repository, if necessary re-add the ssh-key to the agent.  ([See here](https://docs.github.com/en/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent?platform=linux)), i.e.\n",
    "\n",
    "```sh\n",
    "eval \"$(ssh-agent -s)\"\n",
    "ssh-add ~/.ssh/id_ed25519\n",
    "```\n",
    "\n",
    "Then activate the correct modules (`2022` and `Anaconda3/2022.05`) and activate the `gvp` source.  Finally, actually run the notebook with \n",
    "\n",
    "```sh\n",
    "jupyter nbconvert \"latency and memory.ipynb\" --to notebook --execute --inplace --allow-errors\n",
    "```\n",
    "\n",
    "Make sure the environment contains the following packages: `ipywidgets`, `jupyter`, `notebook`, `torch`, `gvp`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "* Import things necessary for running GVP and SMLP, \n",
    "* Setup which task we analyse, \n",
    "* ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T11:58:42.110631Z",
     "iopub.status.busy": "2023-05-26T11:58:42.110036Z",
     "iopub.status.idle": "2023-05-26T11:58:42.121784Z",
     "shell.execute_reply": "2023-05-26T11:58:42.120968Z"
    }
   },
   "outputs": [],
   "source": [
    "# Supresses the following warning:\n",
    "#   UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class.\n",
    "#   This should only matter to you if you are using storages directly.\n",
    "#   To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T11:58:42.126273Z",
     "iopub.status.busy": "2023-05-26T11:58:42.125895Z",
     "iopub.status.idle": "2023-05-26T11:58:50.650627Z",
     "shell.execute_reply": "2023-05-26T11:58:50.649495Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "# import torch.nn as nn\n",
    "\n",
    "from e3nn.o3 import Irreps\n",
    "from torch.profiler import profile, ProfilerActivity\n",
    "\n",
    "from run_atom3d import get_datasets, get_model\n",
    "from steerable_mlp import ConvModel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T11:58:50.657864Z",
     "iopub.status.busy": "2023-05-26T11:58:50.656730Z",
     "iopub.status.idle": "2023-05-26T11:58:50.699670Z",
     "shell.execute_reply": "2023-05-26T11:58:50.698302Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data/\"\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if device != \"cuda\":\n",
    "    exit()\n",
    "\n",
    "TASK = \"LBA\"\n",
    "\"\"\"Task to test for model latency.\"\"\"\n",
    "LBA_SPLIT = 30\n",
    "# SPLIT = 60\n",
    "\n",
    "DATASET_SPLIT = \"train\"\n",
    "\"\"\"Dataset partition from which the sample is chosen.\"\"\"\n",
    "# Select from these values\n",
    "DATASET_SPLIT = [\"train\", \"val\", \"test\"].index(DATASET_SPLIT)\n",
    "SAMPLE_INDEX = 10\n",
    "\"\"\"Dataset index to use for testing the latency and memory.\"\"\"\n",
    "\n",
    "BEST_GVP_MODEL = \"./best_models/LBA_lba-split=30_47.pt\"\n",
    "\"\"\"Path to the best trained relevant model.\"\"\"\n",
    "BEST_SMLP_MODEL = \"./sMLPmodels/LBA-lba_split=30-epoch=10.ckpt\"\n",
    "\"\"\"Path to the best trained relevant model.\"\"\"\n",
    "USE_DENSE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T11:58:50.705493Z",
     "iopub.status.busy": "2023-05-26T11:58:50.704921Z",
     "iopub.status.idle": "2023-05-26T11:58:52.497089Z",
     "shell.execute_reply": "2023-05-26T11:58:52.496649Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[551, 3], edge_index=[2, 13194], atoms=[551], edge_s=[13194, 16], edge_v=[13194, 1, 3], label=6.85, lig_flag=[551], batch=[551], z=[551], pos=[551, 3])\n"
     ]
    }
   ],
   "source": [
    "datasets = get_datasets(TASK, DATA_DIR, LBA_SPLIT)\n",
    "\n",
    "gvp_model = get_model(TASK).to(device)\n",
    "# gvp_model = nn.DataParallel(gvp_model)  # Add parallel to fix OOM issues?\n",
    "gvp_model.load_state_dict(torch.load(BEST_GVP_MODEL), strict=False)\n",
    "\n",
    "dataset_sample = datasets[DATASET_SPLIT][SAMPLE_INDEX].to(device)\n",
    "# This is required for taking the scattermean of the graph\n",
    "dataset_sample.batch = torch.zeros_like(dataset_sample.atoms)\n",
    "# Fix for our steerable.\n",
    "dataset_sample.z = dataset_sample.atoms\n",
    "dataset_sample.pos = dataset_sample.x\n",
    "\n",
    "print(dataset_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T11:58:52.534153Z",
     "iopub.status.busy": "2023-05-26T11:58:52.533717Z",
     "iopub.status.idle": "2023-05-26T11:58:53.095827Z",
     "shell.execute_reply": "2023-05-26T11:58:53.094740Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['embedder.weight', 'layers.0.conv.tp.weight', 'layers.0.conv.tp.output_mask', 'layers.0.conv.radial_net.net.0.freq', 'layers.0.conv.radial_net.net.1.weight', 'layers.0.conv.radial_net.net.1.bias', 'layers.0.conv.radial_net.net.3.weight', 'layers.0.conv.radial_net.net.3.bias', 'layers.0.gate.mul.weight', 'layers.0.gate.mul.output_mask', 'layers.1.conv.tp.weight', 'layers.1.conv.tp.output_mask', 'layers.1.conv.radial_net.net.0.freq', 'layers.1.conv.radial_net.net.1.weight', 'layers.1.conv.radial_net.net.1.bias', 'layers.1.conv.radial_net.net.3.weight', 'layers.1.conv.radial_net.net.3.bias', 'layers.1.gate.mul.weight', 'layers.1.gate.mul.output_mask', 'layers.2.conv.tp.weight', 'layers.2.conv.tp.output_mask', 'layers.2.conv.radial_net.net.0.freq', 'layers.2.conv.radial_net.net.1.weight', 'layers.2.conv.radial_net.net.1.bias', 'layers.2.conv.radial_net.net.3.weight', 'layers.2.conv.radial_net.net.3.bias'], unexpected_keys=['epoch', 'global_step', 'pytorch-lightning_version', 'state_dict', 'loops', 'callbacks', 'optimizer_states', 'lr_schedulers'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def balanced_irreps(hidden_features: int, lmax: int) -> Irreps:\n",
    "    \"\"\"Divide subspaces equally over the feature budget\"\"\"\n",
    "    N = int(hidden_features / (lmax + 1))\n",
    "    irreps = []\n",
    "    for l, irrep in enumerate(Irreps.spherical_harmonics(lmax)):\n",
    "        n = int(N / (2 * l + 1))\n",
    "        irreps.append(str(n) + \"x\" + str(irrep[1]))\n",
    "    irreps = \"+\".join(irreps)\n",
    "    irreps = Irreps(irreps)\n",
    "    gap = hidden_features - irreps.dim\n",
    "    if gap > 0:\n",
    "        irreps = Irreps(\"{}x0e\".format(gap)) + irreps\n",
    "        irreps = irreps.simplify()\n",
    "    return irreps\n",
    "\n",
    "irreps_in = Irreps(\"32x0e\")\n",
    "irreps_hidden = balanced_irreps(128, 1)\n",
    "irreps_edge = Irreps.spherical_harmonics(1)\n",
    "irreps_out = Irreps(\"16x0e\") if USE_DENSE else Irreps(\"1x0e\")\n",
    "smlp_model = ConvModel(\n",
    "    irreps_in=irreps_in,\n",
    "    irreps_hidden=irreps_hidden,\n",
    "    irreps_edge=irreps_edge,\n",
    "    irreps_out=irreps_out,\n",
    "    depth=3,\n",
    "    dense=USE_DENSE,\n",
    ").to(device)\n",
    "smlp_model.load_state_dict(torch.load(BEST_SMLP_MODEL), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T11:58:53.101022Z",
     "iopub.status.busy": "2023-05-26T11:58:53.100552Z",
     "iopub.status.idle": "2023-05-26T11:59:04.271274Z",
     "shell.execute_reply": "2023-05-26T11:59:04.270289Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2023-05-26 13:58:53 16604:16604 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-26 13:58:56 16604:16604 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-26 13:58:56 16604:16604 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "# Burn a few times to prevent extra overhead for first CUDA profiling\n",
    "with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "             profile_memory=True) as prof:\n",
    "    for _ in range(30):\n",
    "        gvp_model(dataset_sample)\n",
    "        smlp_model(dataset_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T11:59:04.275869Z",
     "iopub.status.busy": "2023-05-26T11:59:04.275575Z",
     "iopub.status.idle": "2023-05-26T11:59:04.279737Z",
     "shell.execute_reply": "2023-05-26T11:59:04.279200Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_latency_memory(model, dataset_sample):\n",
    "    memory_start = torch.cuda.memory_allocated()\n",
    "    latencies = []\n",
    "    for _ in range(100):\n",
    "        latency_start = time.perf_counter()\n",
    "        out = model(dataset_sample)\n",
    "        latency_end = time.perf_counter()\n",
    "        latencies.append(latency_end - latency_start)\n",
    "    memory_end = torch.cuda.memory_allocated()\n",
    "    return latencies, memory_end - memory_start"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GVP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T11:59:04.283460Z",
     "iopub.status.busy": "2023-05-26T11:59:04.283196Z",
     "iopub.status.idle": "2023-05-26T11:59:46.780555Z",
     "shell.execute_reply": "2023-05-26T11:59:46.779955Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2023-05-26 13:59:04 16604:16604 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-26 13:59:07 16604:16604 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-26 13:59:07 16604:16604 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-26 13:59:26 16604:16604 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-26 13:59:28 16604:16604 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-26 13:59:29 16604:16604 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.mean(withgrad_latency)=0.024698157098609955\n",
      "np.std(withgrad_latency)=0.0011156942033995645\n",
      "np.mean(gradless_latency)=0.01930151361506432\n",
      "np.std(gradless_latency)=0.002001638903043534\n",
      "\n",
      "withgrad_memory=486084096\n",
      "gradless_memory=512\n"
     ]
    }
   ],
   "source": [
    "gvp_model.train()\n",
    "with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "             profile_memory=True) as prof_train:\n",
    "    withgrad_latency, withgrad_memory = test_latency_memory(gvp_model, dataset_sample)\n",
    "    # out = gvp_model(dataset_sample)\n",
    "\n",
    "\n",
    "gvp_model.eval()\n",
    "with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "            profile_memory=True) as prof_test:\n",
    "    with torch.no_grad():\n",
    "        gradless_latency, gradless_memory = test_latency_memory(gvp_model, dataset_sample)\n",
    "        # out = gvp_model(dataset_sample)\n",
    "\n",
    "print(f\"{np.mean(withgrad_latency)=}\")\n",
    "print(f\"{np.std(withgrad_latency)=}\")\n",
    "print(f\"{np.mean(gradless_latency)=}\")\n",
    "print(f\"{np.std(gradless_latency)=}\")\n",
    "print()\n",
    "print(f\"{withgrad_memory=}\")\n",
    "print(f\"{gradless_memory=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T11:59:46.785081Z",
     "iopub.status.busy": "2023-05-26T11:59:46.784811Z",
     "iopub.status.idle": "2023-05-26T11:59:46.789851Z",
     "shell.execute_reply": "2023-05-26T11:59:46.788874Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "withgrad_latency=[0.026040896074846387, 0.030528782168403268, 0.02490377682261169, 0.024250203976407647, 0.024611416971310973, 0.023828176083043218, 0.024133834056556225, 0.025092604802921414, 0.025831161998212337, 0.02391382586210966, 0.024131011916324496, 0.024148999946191907, 0.0241327250842005, 0.02377885510213673, 0.024043763056397438, 0.024459959007799625, 0.027284455951303244, 0.02391892788000405, 0.024027460953220725, 0.02448016102425754, 0.02417381713166833, 0.023691244889050722, 0.024085141951218247, 0.02426473516970873, 0.02725474303588271, 0.02418029890395701, 0.025153805036097765, 0.02556224400177598, 0.025014425860717893, 0.02392437681555748, 0.024515022058039904, 0.024441973073408008, 0.027083539171144366, 0.02395139099098742, 0.026500088861212134, 0.02435100614093244, 0.024559972807765007, 0.023913527838885784, 0.024194865953177214, 0.02421195711940527, 0.027140071149915457, 0.024123532930389047, 0.024464973947033286, 0.024243619991466403, 0.024427304044365883, 0.02402613591402769, 0.02413771697320044, 0.02424176107160747, 0.027280923910439014, 0.024151307065039873, 0.02435380802489817, 0.02427900186739862, 0.02427428006194532, 0.02379703288897872, 0.024198834085837007, 0.024193892953917384, 0.027262382907792926, 0.024199208011850715, 0.0243138549849391, 0.024278784869238734, 0.024195728125050664, 0.0238331810105592, 0.024152284022420645, 0.024238609010353684, 0.024224051041528583, 0.026687517995014787, 0.02450625505298376, 0.02438222896307707, 0.024264347041025758, 0.023822444025427103, 0.024237934965640306, 0.024304302874952555, 0.024244484025985003, 0.026641489006578922, 0.024479118874296546, 0.024572861148044467, 0.0242900880984962, 0.023758677067235112, 0.024083069059997797, 0.024258927209302783, 0.02418400696478784, 0.026651266030967236, 0.02439112588763237, 0.024645685916766524, 0.02420287416316569, 0.023789678001776338, 0.024092697072774172, 0.024468342075124383, 0.024195672944188118, 0.02663588896393776, 0.024529685033485293, 0.024380280869081616, 0.024281830061227083, 0.023796613095328212, 0.02414139313623309, 0.024220002116635442, 0.024754985002800822, 0.026064637815579772, 0.026817291975021362, 0.024436550913378596]\n",
      "gradless_latency=[0.037769834976643324, 0.018905889941379428, 0.018782895989716053, 0.018725028028711677, 0.018676650011911988, 0.018673164071515203, 0.0186519508715719, 0.019092126050963998, 0.018655095947906375, 0.021083777071908116, 0.019926541950553656, 0.019997315015643835, 0.019405042054131627, 0.019115667091682553, 0.018677464919164777, 0.01869196887128055, 0.0186932859942317, 0.01884395908564329, 0.019941463135182858, 0.01961967209354043, 0.018894555047154427, 0.018693864811211824, 0.018870275001972914, 0.018654913874343038, 0.018862708006054163, 0.01865632995031774, 0.01870733709074557, 0.019999467069283128, 0.01880351803265512, 0.01870070700533688, 0.018705981085076928, 0.01868189312517643, 0.018657817039638758, 0.018813110189512372, 0.018892653053626418, 0.019174537155777216, 0.020099379122257233, 0.018802029080688953, 0.01872137002646923, 0.018673006212338805, 0.019362103892490268, 0.021400119876489043, 0.02184824296273291, 0.02046235790476203, 0.019596707075834274, 0.021485770121216774, 0.019299541134387255, 0.019349654903635383, 0.01950698788277805, 0.01885232306085527, 0.018661273876205087, 0.018661357928067446, 0.01866141310892999, 0.01865089894272387, 0.01981430989690125, 0.01893877098336816, 0.018686993047595024, 0.018637022003531456, 0.018628454068675637, 0.01875146897509694, 0.01864516898058355, 0.018627540906891227, 0.018807876156643033, 0.020573407877236605, 0.01951681892387569, 0.018912226194515824, 0.018646675860509276, 0.019027802860364318, 0.01860459102317691, 0.018772752955555916, 0.018628926947712898, 0.018642478855326772, 0.018647102173417807, 0.02123256609775126, 0.019111305009573698, 0.018665478099137545, 0.01865373714827001, 0.018684187904000282, 0.019360374892130494, 0.019135026959702373, 0.019303784938529134, 0.01900296611711383, 0.0214325119741261, 0.0189479470718652, 0.01883135410025716, 0.018949505873024464, 0.018618685891851783, 0.01861836202442646, 0.018592491978779435, 0.01879056799225509, 0.01863650302402675, 0.020968311931937933, 0.018794388975948095, 0.01864751218818128, 0.018646860029548407, 0.018597529968246818, 0.018599222879856825, 0.0186016580555588, 0.018633188912644982, 0.01879194495268166]\n"
     ]
    }
   ],
   "source": [
    "print(f\"{withgrad_latency=}\")\n",
    "print(f\"{gradless_latency=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T11:59:46.793128Z",
     "iopub.status.busy": "2023-05-26T11:59:46.792585Z",
     "iopub.status.idle": "2023-05-26T12:00:01.561738Z",
     "shell.execute_reply": "2023-05-26T12:00:01.560885Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================================================================================================================================================================================================================================\n",
      "This report only display top-level ops statistics\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                           aten::linear         2.78%      49.864ms        34.58%     619.394ms      56.825us       0.000us         0.00%     568.331ms      52.140us           0 b           0 b      18.18 Gb     589.59 Mb         10900  \n",
      "                                   volta_sgemm_64x64_tt         0.00%       0.000us         0.00%       0.000us       0.000us     241.761ms        26.28%     241.761ms     219.783us           0 b           0 b           0 b           0 b          1100  \n",
      "                                  volta_sgemm_128x64_tn         0.00%       0.000us         0.00%       0.000us       0.000us     181.331ms        19.71%     181.331ms      90.665us           0 b           0 b           0 b           0 b          2000  \n",
      "                                              aten::cat         3.55%      63.666ms         5.11%      91.469ms      21.778us     126.821ms        13.78%     128.976ms      30.709us           0 b           0 b      24.72 Gb      24.72 Gb          4200  \n",
      "                                  volta_sgemm_32x128_tn         0.00%       0.000us         0.00%       0.000us       0.000us     117.549ms        12.78%     117.549ms      25.554us           0 b           0 b           0 b           0 b          4600  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us     112.449ms        12.22%     112.449ms      30.392us           0 b           0 b           0 b           0 b          3700  \n",
      "void at::native::(anonymous namespace)::indexSelectL...         0.00%       0.000us         0.00%       0.000us       0.000us      34.095ms         3.71%      34.095ms      16.236us           0 b           0 b           0 b           0 b          2100  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      33.012ms         3.59%      33.012ms       5.412us           0 b           0 b           0 b           0 b          6100  \n",
      "                                              aten::sum         3.88%      69.435ms         5.07%      90.761ms      23.272us      29.347ms         3.19%      31.024ms       7.955us           0 b           0 b       1.77 Gb       1.77 Gb          3900  \n",
      "                                              aten::mul         4.07%      72.955ms         5.54%      99.158ms      21.556us      23.323ms         2.53%      26.509ms       5.763us           0 b           0 b       3.89 Gb       3.89 Gb          4600  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.791s\n",
      "Self CUDA time total: 920.063ms\n",
      "\n",
      "===========================================================================================================================================================================================================================================================\n",
      "This report only display top-level ops statistics\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                              aten::cat         3.55%      63.666ms         5.11%      91.469ms      21.778us     126.821ms        13.78%     128.976ms      30.709us           0 b           0 b      24.72 Gb      24.72 Gb          4200  \n",
      "                                           aten::linear         2.78%      49.864ms        34.58%     619.394ms      56.825us       0.000us         0.00%     568.331ms      52.140us           0 b           0 b      18.18 Gb     589.59 Mb         10900  \n",
      "                                             aten::relu         0.58%      10.312ms         2.03%      36.293ms      22.683us       0.000us         0.00%      22.508ms      14.068us           0 b           0 b       5.37 Gb           0 b          1600  \n",
      "                                           aten::square         0.30%       5.420ms         5.13%      91.903ms      23.565us       0.000us         0.00%      21.412ms       5.490us           0 b           0 b       5.19 Gb      22.92 Mb          3900  \n",
      "                                              aten::mul         4.07%      72.955ms         5.54%      99.158ms      21.556us      23.323ms         2.53%      26.509ms       5.763us           0 b           0 b       3.89 Gb       3.89 Gb          4600  \n",
      "                                            aten::clamp         3.26%      58.376ms         4.63%      82.881ms      18.837us      10.834ms         1.18%      12.616ms       2.867us           0 b           0 b       1.80 Gb       1.80 Gb          4400  \n",
      "                                             aten::sqrt         2.58%      46.146ms         3.72%      66.612ms      17.080us       9.846ms         1.07%      11.510ms       2.951us           0 b           0 b       1.78 Gb       1.78 Gb          3900  \n",
      "                                              aten::sum         3.88%      69.435ms         5.07%      90.761ms      23.272us      29.347ms         3.19%      31.024ms       7.955us           0 b           0 b       1.77 Gb       1.77 Gb          3900  \n",
      "                                          aten::sigmoid         2.14%      38.323ms         3.00%      53.740ms      20.669us       6.895ms         0.75%       8.585ms       3.302us           0 b           0 b       1.23 Gb       1.23 Gb          2600  \n",
      "                                              aten::add         2.43%      43.451ms         3.63%      64.928ms      16.232us       8.889ms         0.97%      11.206ms       2.801us          -8 b          -8 b     623.05 Mb     623.05 Mb          4000  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.791s\n",
      "Self CUDA time total: 920.063ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prof_train.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10, top_level_events_only=True))\n",
    "print(prof_train.key_averages().table(sort_by=\"cuda_memory_usage\", row_limit=10, top_level_events_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T12:00:01.566786Z",
     "iopub.status.busy": "2023-05-26T12:00:01.566438Z",
     "iopub.status.idle": "2023-05-26T12:00:14.716346Z",
     "shell.execute_reply": "2023-05-26T12:00:14.715071Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================================================================================================================================================================================================================================\n",
      "This report only display top-level ops statistics\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                           aten::linear         3.04%      42.559ms        37.00%     517.518ms      47.479us       0.000us         0.00%     471.874ms      43.291us           0 b           0 b      18.04 Gb     602.12 Mb         10900  \n",
      "                                   volta_sgemm_64x64_tt         0.00%       0.000us         0.00%       0.000us       0.000us     205.029ms        25.57%     205.029ms     186.390us           0 b           0 b           0 b           0 b          1100  \n",
      "                                  volta_sgemm_128x64_tn         0.00%       0.000us         0.00%       0.000us       0.000us     156.876ms        19.57%     156.876ms      78.438us           0 b           0 b           0 b           0 b          2000  \n",
      "                                              aten::cat         3.63%      50.841ms         5.55%      77.648ms      18.488us     125.557ms        15.66%     125.557ms      29.895us           0 b           0 b      24.67 Gb      24.67 Gb          4200  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us     111.371ms        13.89%     111.371ms      30.100us           0 b           0 b           0 b           0 b          3700  \n",
      "                                  volta_sgemm_32x128_tn         0.00%       0.000us         0.00%       0.000us       0.000us     105.254ms        13.13%     105.254ms      22.881us           0 b           0 b           0 b           0 b          4600  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      31.202ms         3.89%      31.202ms       5.115us           0 b           0 b           0 b           0 b          6100  \n",
      "void at::native::(anonymous namespace)::indexSelectL...         0.00%       0.000us         0.00%       0.000us       0.000us      29.631ms         3.70%      29.631ms      14.110us           0 b           0 b           0 b           0 b          2100  \n",
      "                                              aten::sum         4.44%      62.091ms         5.91%      82.599ms      21.179us      25.468ms         3.18%      25.468ms       6.530us           0 b           0 b       2.04 Gb       2.04 Gb          3900  \n",
      "                                             aten::relu         0.45%       6.255ms         2.12%      29.714ms      18.571us       0.000us         0.00%      21.518ms      13.449us           0 b           0 b       5.35 Gb           0 b          1600  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.399s\n",
      "Self CUDA time total: 801.797ms\n",
      "\n",
      "===========================================================================================================================================================================================================================================================\n",
      "This report only display top-level ops statistics\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                              aten::cat         3.63%      50.841ms         5.55%      77.648ms      18.488us     125.557ms        15.66%     125.557ms      29.895us           0 b           0 b      24.67 Gb      24.67 Gb          4200  \n",
      "                                           aten::linear         3.04%      42.559ms        37.00%     517.518ms      47.479us       0.000us         0.00%     471.874ms      43.291us           0 b           0 b      18.04 Gb     602.12 Mb         10900  \n",
      "                                             aten::relu         0.45%       6.255ms         2.12%      29.714ms      18.571us       0.000us         0.00%      21.518ms      13.449us           0 b           0 b       5.35 Gb           0 b          1600  \n",
      "                                           aten::square         0.40%       5.581ms         5.81%      81.217ms      20.825us       0.000us         0.00%      18.998ms       4.871us           0 b           0 b       5.14 Gb      26.18 Mb          3900  \n",
      "                                              aten::mul         2.55%      35.711ms         3.66%      51.225ms      19.702us      16.345ms         2.04%      16.345ms       6.287us           0 b           0 b       3.74 Gb       3.74 Gb          2600  \n",
      "                                              aten::sum         4.44%      62.091ms         5.91%      82.599ms      21.179us      25.468ms         3.18%      25.468ms       6.530us           0 b           0 b       2.04 Gb       2.04 Gb          3900  \n",
      "                                             aten::sqrt         2.39%      33.442ms         3.78%      52.808ms      13.541us       8.473ms         1.06%       8.473ms       2.173us           0 b           0 b       2.00 Gb       2.00 Gb          3900  \n",
      "                                            aten::clamp         3.68%      51.421ms         5.32%      74.430ms      16.916us       9.484ms         1.18%       9.484ms       2.155us           0 b           0 b       1.71 Gb       1.71 Gb          4400  \n",
      "                                          aten::sigmoid         2.06%      28.800ms         3.09%      43.259ms      16.638us       6.670ms         0.83%       6.670ms       2.565us           0 b           0 b       1.23 Gb       1.23 Gb          2600  \n",
      "                                              aten::add         2.99%      41.797ms         4.59%      64.264ms      16.066us       8.233ms         1.03%       8.233ms       2.058us           0 b           0 b     623.05 Mb     623.05 Mb          4000  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.399s\n",
      "Self CUDA time total: 801.797ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prof_test.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10, top_level_events_only=True))\n",
    "print(prof_test.key_averages().table(sort_by=\"cuda_memory_usage\", row_limit=10, top_level_events_only=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steerable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T12:00:14.722310Z",
     "iopub.status.busy": "2023-05-26T12:00:14.721442Z",
     "iopub.status.idle": "2023-05-26T12:00:33.892698Z",
     "shell.execute_reply": "2023-05-26T12:00:33.891340Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2023-05-26 14:00:14 16604:16604 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-26 14:00:15 16604:16604 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-26 14:00:15 16604:16604 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-26 14:00:24 16604:16604 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-26 14:00:25 16604:16604 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-26 14:00:25 16604:16604 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.mean(withgrad_latency)=0.006889338770415634\n",
      "np.std(withgrad_latency)=0.0018428532743030978\n",
      "np.mean(gradless_latency)=0.005986865872982889\n",
      "np.std(gradless_latency)=0.00036821399083455513\n",
      "\n",
      "withgrad_memory=706749952\n",
      "gradless_memory=512\n"
     ]
    }
   ],
   "source": [
    "gvp_model.train()\n",
    "with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "             profile_memory=True) as prof_train:\n",
    "    withgrad_latency, withgrad_memory = test_latency_memory(smlp_model, dataset_sample)\n",
    "    # out = gvp_model(dataset_sample)\n",
    "\n",
    "\n",
    "gvp_model.eval()\n",
    "with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "            profile_memory=True) as prof_test:\n",
    "    with torch.no_grad():\n",
    "        gradless_latency, gradless_memory = test_latency_memory(smlp_model, dataset_sample)\n",
    "        # out = gvp_model(dataset_sample)\n",
    "\n",
    "print(f\"{np.mean(withgrad_latency)=}\")\n",
    "print(f\"{np.std(withgrad_latency)=}\")\n",
    "print(f\"{np.mean(gradless_latency)=}\")\n",
    "print(f\"{np.std(gradless_latency)=}\")\n",
    "print()\n",
    "print(f\"{withgrad_memory=}\")\n",
    "print(f\"{gradless_memory=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T12:00:33.898447Z",
     "iopub.status.busy": "2023-05-26T12:00:33.897637Z",
     "iopub.status.idle": "2023-05-26T12:00:33.904645Z",
     "shell.execute_reply": "2023-05-26T12:00:33.903538Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "withgrad_latency=[0.024855504045262933, 0.008508198894560337, 0.006764301098883152, 0.006735835922881961, 0.006660043029114604, 0.0066592448856681585, 0.006657347083091736, 0.006586449919268489, 0.006643848028033972, 0.006682778941467404, 0.006601940840482712, 0.006634454010054469, 0.006640654988586903, 0.006620875094085932, 0.0066335059236735106, 0.006654773838818073, 0.006619638996198773, 0.006591200130060315, 0.00656069815158844, 0.006580753950402141, 0.006620119092985988, 0.006598950829356909, 0.006587903015315533, 0.006698202108964324, 0.00660217204131186, 0.006719080964103341, 0.0066456361673772335, 0.007667589001357555, 0.006654075114056468, 0.006620369851589203, 0.006602704990655184, 0.006595744052901864, 0.006620327010750771, 0.006526427110657096, 0.00806512008421123, 0.00676415185444057, 0.006743261124938726, 0.006637979997321963, 0.006595023209229112, 0.006599198095500469, 0.006670881062746048, 0.006608510855585337, 0.006728142965584993, 0.006622391054406762, 0.0067000319249928, 0.0068597940262407064, 0.006600025808438659, 0.006616011029109359, 0.006701906910166144, 0.006612997967749834, 0.0065979547798633575, 0.006605455884709954, 0.006610244046896696, 0.006596239982172847, 0.0066138338297605515, 0.006605150876566768, 0.006724793929606676, 0.006615346064791083, 0.00664315209724009, 0.0065368060022592545, 0.006591154029592872, 0.006599717075005174, 0.00676959496922791, 0.006716056959703565, 0.006606428883969784, 0.006597819039598107, 0.006591906072571874, 0.0065990169532597065, 0.00929062208160758, 0.006835829000920057, 0.006609952775761485, 0.006658988073468208, 0.006633718963712454, 0.006581481080502272, 0.006603432120755315, 0.006607003975659609, 0.006602524081245065, 0.006729395827278495, 0.006898835068568587, 0.006714806193485856, 0.006621187087148428, 0.006564003182575107, 0.006577288964763284, 0.006584058050066233, 0.006716493051499128, 0.00655609997920692, 0.006574475904926658, 0.0065837770234793425, 0.00656944396905601, 0.006612672004848719, 0.00662378896959126, 0.006615737918764353, 0.006714090937748551, 0.006615508114919066, 0.00661213300190866, 0.006626499816775322, 0.006567558040842414, 0.006620113970711827, 0.006636143196374178, 0.006578766042366624]\n",
      "gradless_latency=[0.007849557092413306, 0.006261949893087149, 0.0061716430354863405, 0.006129509070888162, 0.006156067131087184, 0.006148137850686908, 0.006127635017037392, 0.0061109368689358234, 0.00609205593355, 0.006113315001130104, 0.006125500192865729, 0.006150250090286136, 0.006086244946345687, 0.006102835992351174, 0.006067231064662337, 0.006122020073235035, 0.006094734882935882, 0.006079511949792504, 0.006077450001612306, 0.006100310944020748, 0.006064227083697915, 0.00597121799364686, 0.005983700044453144, 0.005924704950302839, 0.005894778994843364, 0.005926812067627907, 0.007625852013006806, 0.0059499540366232395, 0.0059309410862624645, 0.005916955880820751, 0.005941416835412383, 0.0059249489568173885, 0.00652745901606977, 0.005968804005533457, 0.007352301152423024, 0.00597404595464468, 0.005981258116662502, 0.005949268816038966, 0.005947768222540617, 0.005921210860833526, 0.005918923998251557, 0.0059312740340828896, 0.005936486879363656, 0.005905818892642856, 0.005913723027333617, 0.005958113120868802, 0.00592293799854815, 0.005890251137316227, 0.0059262970462441444, 0.00593001302331686, 0.005913039902225137, 0.005909482948482037, 0.005916217807680368, 0.00591196003369987, 0.005928248865529895, 0.005915630143135786, 0.0058385091833770275, 0.005777298007160425, 0.005766605958342552, 0.005911047803238034, 0.005762954009696841, 0.005776148987933993, 0.005761152133345604, 0.005775905912742019, 0.005763599881902337, 0.005786141147837043, 0.0057500069960951805, 0.0058323959819972515, 0.007744929054751992, 0.006093160016462207, 0.005800005979835987, 0.005803360138088465, 0.005764300003647804, 0.0057985608000308275, 0.005773377139121294, 0.005806638160720468, 0.005787526024505496, 0.005771968979388475, 0.005782520864158869, 0.005796646932139993, 0.005808585789054632, 0.005767734022811055, 0.005783496191725135, 0.005781678948551416, 0.005778250051662326, 0.00576625089161098, 0.005790238035842776, 0.005783521104604006, 0.00576513004489243, 0.005805853055790067, 0.00593651388771832, 0.005804912885650992, 0.0057702341582626104, 0.005800365004688501, 0.005785734858363867, 0.0058032451197505, 0.005765510955825448, 0.005787264090031385, 0.005789249204099178, 0.005815116921439767]\n"
     ]
    }
   ],
   "source": [
    "print(f\"{withgrad_latency=}\")\n",
    "print(f\"{gradless_latency=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T12:00:33.909025Z",
     "iopub.status.busy": "2023-05-26T12:00:33.908471Z",
     "iopub.status.idle": "2023-05-26T12:00:39.609004Z",
     "shell.execute_reply": "2023-05-26T12:00:39.608226Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================================================================================================================================================================================================================================\n",
      "This report only display top-level ops statistics\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                forward         6.03%      31.595ms        52.15%     273.027ms     390.039us       0.000us         0.00%     202.191ms     288.844us           0 b        -696 b       2.53 Gb    -617.69 Mb           700  \n",
      "                                           aten::linear         0.44%       2.284ms         7.32%      38.337ms      63.895us       0.000us         0.00%     143.695ms     239.492us           0 b           0 b      62.81 Gb       1.77 Gb           600  \n",
      "                                 volta_sgemm_128x128_tn         0.00%       0.000us         0.00%       0.000us       0.000us      98.022ms        25.45%      98.022ms     980.220us           0 b           0 b           0 b           0 b           100  \n",
      "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us      53.908ms        13.99%      53.908ms      89.847us           0 b           0 b           0 b           0 b           600  \n",
      "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us      42.878ms        11.13%      42.878ms     428.780us           0 b           0 b           0 b           0 b           100  \n",
      "void gemv2N_kernel<int, int, float, float, float, fl...         0.00%       0.000us         0.00%       0.000us       0.000us      37.947ms         9.85%      37.947ms     189.735us           0 b           0 b           0 b           0 b           200  \n",
      "                                  volta_sgemm_128x64_tn         0.00%       0.000us         0.00%       0.000us       0.000us      36.130ms         9.38%      36.130ms     361.300us           0 b           0 b           0 b           0 b           100  \n",
      "                                   volta_sgemm_64x64_nt         0.00%       0.000us         0.00%       0.000us       0.000us      32.238ms         8.37%      32.238ms     322.380us           0 b           0 b           0 b           0 b           100  \n",
      "                                  volta_sgemm_32x128_tn         0.00%       0.000us         0.00%       0.000us       0.000us      13.652ms         3.54%      13.652ms      34.130us           0 b           0 b           0 b           0 b           400  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us      10.199ms         2.65%      10.199ms      20.398us           0 b           0 b           0 b           0 b           500  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 523.569ms\n",
      "Self CUDA time total: 385.207ms\n",
      "\n",
      "===========================================================================================================================================================================================================================================================\n",
      "This report only display top-level ops statistics\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                           aten::linear         0.44%       2.284ms         7.32%      38.337ms      63.895us       0.000us         0.00%     143.695ms     239.492us           0 b           0 b      62.81 Gb       1.77 Gb           600  \n",
      "                                                forward         6.03%      31.595ms        52.15%     273.027ms     390.039us       0.000us         0.00%     202.191ms     288.844us           0 b        -696 b       2.53 Gb    -617.69 Mb           700  \n",
      "                                             aten::silu         1.25%       6.537ms         1.81%       9.465ms      18.930us       1.390ms         0.36%       1.390ms       2.780us           0 b           0 b     269.04 Mb     269.04 Mb           500  \n",
      "                                              aten::sin         0.56%       2.950ms         0.84%       4.412ms      14.707us     800.000us         0.21%     800.000us       2.667us           0 b           0 b     151.03 Mb     151.03 Mb           300  \n",
      "                                        aten::new_zeros         0.62%       3.225ms         4.84%      25.365ms      23.059us       0.000us         0.00%       1.862ms       1.693us           0 b           0 b     125.83 Mb      -7.91 Mb          1100  \n",
      "                                              aten::div         1.23%       6.421ms         1.79%       9.373ms      18.746us       1.123ms         0.29%       1.123ms       2.246us           0 b           0 b      30.42 Mb      30.42 Mb           500  \n",
      "                                            aten::index         0.66%       3.460ms         3.84%      20.091ms     100.455us       1.065ms         0.28%       1.065ms       5.325us           0 b           0 b      30.27 Mb      30.27 Mb           200  \n",
      "                                   _spherical_harmonics         0.30%       1.556ms         2.23%      11.673ms     116.730us       0.000us         0.00%       1.240ms      12.400us           0 b        -744 b      20.17 Mb     -20.31 Mb           100  \n",
      "                                              aten::pow         0.78%       4.090ms         1.09%       5.702ms      19.007us       1.021ms         0.27%       1.021ms       3.403us           0 b           0 b      15.23 Mb      15.23 Mb           300  \n",
      "                                       aten::reciprocal         0.48%       2.494ms         0.77%       4.021ms      13.403us     618.000us         0.16%     618.000us       2.060us           0 b           0 b      15.23 Mb      15.23 Mb           300  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 523.569ms\n",
      "Self CUDA time total: 385.207ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prof_train.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10, top_level_events_only=True))\n",
    "print(prof_train.key_averages().table(sort_by=\"cuda_memory_usage\", row_limit=10, top_level_events_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T12:00:39.613700Z",
     "iopub.status.busy": "2023-05-26T12:00:39.613368Z",
     "iopub.status.idle": "2023-05-26T12:00:45.302425Z",
     "shell.execute_reply": "2023-05-26T12:00:45.301728Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================================================================================================================================================================================================================================\n",
      "This report only display top-level ops statistics\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                forward         6.60%      30.224ms        50.75%     232.283ms     331.833us       0.000us         0.00%     201.313ms     287.590us           0 b        -600 b       1.56 Gb    -956.48 Mb           700  \n",
      "                                           aten::linear         0.49%       2.232ms         7.19%      32.905ms      54.842us       0.000us         0.00%     148.554ms     247.590us           0 b           0 b      62.80 Gb       5.94 Mb           600  \n",
      "                                 volta_sgemm_128x128_tn         0.00%       0.000us         0.00%       0.000us       0.000us      98.870ms        25.69%      98.870ms     988.700us           0 b           0 b           0 b           0 b           100  \n",
      "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us      53.586ms        13.92%      53.586ms      89.310us           0 b           0 b           0 b           0 b           600  \n",
      "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us      42.868ms        11.14%      42.868ms     428.680us           0 b           0 b           0 b           0 b           100  \n",
      "void gemv2N_kernel<int, int, float, float, float, fl...         0.00%       0.000us         0.00%       0.000us       0.000us      37.845ms         9.83%      37.845ms     189.225us           0 b           0 b           0 b           0 b           200  \n",
      "                                  volta_sgemm_128x64_tn         0.00%       0.000us         0.00%       0.000us       0.000us      36.160ms         9.40%      36.160ms     361.600us           0 b           0 b           0 b           0 b           100  \n",
      "                                   volta_sgemm_64x64_nt         0.00%       0.000us         0.00%       0.000us       0.000us      32.129ms         8.35%      32.129ms     321.290us           0 b           0 b           0 b           0 b           100  \n",
      "                                  volta_sgemm_32x128_tn         0.00%       0.000us         0.00%       0.000us       0.000us      13.654ms         3.55%      13.654ms      34.135us           0 b           0 b           0 b           0 b           400  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us       9.817ms         2.55%       9.817ms      19.634us           0 b           0 b           0 b           0 b           500  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 457.663ms\n",
      "Self CUDA time total: 384.867ms\n",
      "\n",
      "===========================================================================================================================================================================================================================================================\n",
      "This report only display top-level ops statistics\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                           aten::linear         0.49%       2.232ms         7.19%      32.905ms      54.842us       0.000us         0.00%     148.554ms     247.590us           0 b           0 b      62.80 Gb       5.94 Mb           600  \n",
      "                                                forward         6.60%      30.224ms        50.75%     232.283ms     331.833us       0.000us         0.00%     201.313ms     287.590us           0 b        -600 b       1.56 Gb    -956.48 Mb           700  \n",
      "                                             aten::silu         1.17%       5.353ms         1.78%       8.150ms      16.300us       1.254ms         0.33%       1.254ms       2.508us           0 b           0 b     269.04 Mb     269.04 Mb           500  \n",
      "                                              aten::sin         0.53%       2.435ms         0.82%       3.764ms      12.547us     829.000us         0.22%     829.000us       2.763us           0 b           0 b     151.03 Mb     151.03 Mb           300  \n",
      "                                        aten::new_zeros         0.77%       3.522ms         5.51%      25.200ms      22.909us       0.000us         0.00%       1.937ms       1.761us           0 b           0 b     125.83 Mb      -5.07 Mb          1100  \n",
      "                                              aten::div         1.30%       5.932ms         1.92%       8.788ms      17.576us       1.163ms         0.30%       1.163ms       2.326us           0 b           0 b      30.42 Mb      30.42 Mb           500  \n",
      "                                            aten::index         0.65%       2.952ms         1.20%       5.470ms      27.350us       1.055ms         0.27%       1.055ms       5.275us           0 b           0 b      30.27 Mb      30.27 Mb           200  \n",
      "                                   _spherical_harmonics         0.32%       1.475ms         2.51%      11.475ms     114.750us       0.000us         0.00%       1.217ms      12.170us           0 b        -568 b      20.17 Mb     -20.31 Mb           100  \n",
      "                                              aten::pow         0.84%       3.859ms         1.19%       5.462ms      18.207us     940.000us         0.24%     940.000us       3.133us           0 b           0 b      15.23 Mb      15.23 Mb           300  \n",
      "                                       aten::reciprocal         0.54%       2.474ms         0.86%       3.953ms      13.177us     604.000us         0.16%     604.000us       2.013us           0 b           0 b      15.23 Mb      15.23 Mb           300  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 457.663ms\n",
      "Self CUDA time total: 384.867ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prof_test.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10, top_level_events_only=True))\n",
    "print(prof_test.key_averages().table(sort_by=\"cuda_memory_usage\", row_limit=10, top_level_events_only=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "It seems that the profiling does not take into account the fact that gradients have to be stored.  We think this, because the additional memory allocated to CUDA is more than 450 MB and more than 700 MB respectively when storing the gradients, but the profiler only gives 250 MB and 630 MB (per iteration) at most."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gvp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
