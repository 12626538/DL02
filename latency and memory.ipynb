{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latency and memory\n",
    "\n",
    "## How to run this file on the lisa cluster\n",
    "\n",
    "Start by pulling the lastest version of the repository, if necessary re-add the ssh-key to the agent.  ([See here](https://docs.github.com/en/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent?platform=linux)), i.e.\n",
    "\n",
    "```sh\n",
    "eval \"$(ssh-agent -s)\"\n",
    "ssh-add ~/.ssh/id_ed25519\n",
    "```\n",
    "\n",
    "Then activate the correct modules (`2022` and `Anaconda3/2022.05`) and activate the `gvp` source.  Finally, actually run the notebook with \n",
    "\n",
    "```sh\n",
    "jupyter nbconvert \"latency and memory.ipynb\" --to notebook --execute --inplace --allow-errors\n",
    "```\n",
    "\n",
    "Make sure the environment contains the following packages: `ipywidgets`, `jupyter`, `notebook`, `torch`, `gvp`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "* Import things necessary for running GVP and SMLP, \n",
    "* Setup which task we analyse, \n",
    "* ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-25T11:53:31.903985Z",
     "iopub.status.busy": "2023-05-25T11:53:31.903212Z",
     "iopub.status.idle": "2023-05-25T11:53:31.915578Z",
     "shell.execute_reply": "2023-05-25T11:53:31.914547Z"
    }
   },
   "outputs": [],
   "source": [
    "# Supresses the following warning:\n",
    "#   UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class.\n",
    "#   This should only matter to you if you are using storages directly.\n",
    "#   To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-25T11:53:31.919684Z",
     "iopub.status.busy": "2023-05-25T11:53:31.919178Z",
     "iopub.status.idle": "2023-05-25T11:53:37.186547Z",
     "shell.execute_reply": "2023-05-25T11:53:37.185920Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from run_atom3d import get_datasets, get_model\n",
    "from torch.profiler import profile, record_function, ProfilerActivity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-25T11:53:37.192251Z",
     "iopub.status.busy": "2023-05-25T11:53:37.191767Z",
     "iopub.status.idle": "2023-05-25T11:53:37.201373Z",
     "shell.execute_reply": "2023-05-25T11:53:37.200633Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Path to the best trained relevant model.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIR = \"../data/\"\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if device != \"cuda\":\n",
    "    exit()\n",
    "\n",
    "TASK = \"LBA\"\n",
    "\"\"\"Task to test for model latency.\"\"\"\n",
    "LBA_SPLIT = 30\n",
    "# SPLIT = 60\n",
    "\n",
    "DATASET_SPLIT = \"train\"\n",
    "\"\"\"Dataset partition from which the sample is chosen.\"\"\"\n",
    "# Select from these values\n",
    "DATASET_SPLIT = [\"train\", \"val\", \"test\"].index(DATASET_SPLIT)\n",
    "SAMPLE_INDEX = 10\n",
    "\"\"\"Dataset index to use for testing the latency and memory.\"\"\"\n",
    "\n",
    "BEST_GVP_MODEL = \"./best_models/LBA_lba-split=30_47.pt\"\n",
    "\"\"\"Path to the best trained relevant model.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-25T11:53:37.236614Z",
     "iopub.status.busy": "2023-05-25T11:53:37.235969Z",
     "iopub.status.idle": "2023-05-25T11:53:39.082298Z",
     "shell.execute_reply": "2023-05-25T11:53:39.081853Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[551, 3], edge_index=[2, 13194], atoms=[551], edge_s=[13194, 16], edge_v=[13194, 1, 3], label=6.85, lig_flag=[551], batch=[551])\n"
     ]
    }
   ],
   "source": [
    "datasets = get_datasets(TASK, DATA_DIR, LBA_SPLIT)\n",
    "\n",
    "gvp_model = get_model(TASK).to(device)\n",
    "# gvp_model = nn.DataParallel(gvp_model)  # Add parallel to fix OOM issues?\n",
    "gvp_model.load_state_dict(torch.load(BEST_GVP_MODEL), strict=False)\n",
    "\n",
    "dataset_sample = datasets[DATASET_SPLIT][SAMPLE_INDEX].to(device)\n",
    "# This is required for taking the scattermean of the graph\n",
    "dataset_sample.batch = torch.zeros_like(dataset_sample.atoms)\n",
    "\n",
    "print(dataset_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-25T11:53:39.085137Z",
     "iopub.status.busy": "2023-05-25T11:53:39.084843Z",
     "iopub.status.idle": "2023-05-25T11:53:40.525514Z",
     "shell.execute_reply": "2023-05-25T11:53:40.524900Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2023-05-25 13:53:39 18352:18352 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-25 13:53:40 18352:18352 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-25 13:53:40 18352:18352 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "# Burn once to prevent extra overhead for first CUDA profiling\n",
    "with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "             profile_memory=True) as prof:\n",
    "    gvp_model(dataset_sample)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latency"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GVP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-25T11:53:40.530736Z",
     "iopub.status.busy": "2023-05-25T11:53:40.530440Z",
     "iopub.status.idle": "2023-05-25T11:53:40.534140Z",
     "shell.execute_reply": "2023-05-25T11:53:40.533601Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_latency_memory(model, dataset_sample):\n",
    "    latency_start = time.perf_counter()\n",
    "    memory_start = torch.cuda.memory_allocated()\n",
    "    out = model(dataset_sample)\n",
    "    latency_end = time.perf_counter()\n",
    "    memory_end = torch.cuda.memory_allocated()\n",
    "    return latency_end - latency_start, memory_end - memory_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-25T11:53:40.546148Z",
     "iopub.status.busy": "2023-05-25T11:53:40.545864Z",
     "iopub.status.idle": "2023-05-25T11:53:40.997364Z",
     "shell.execute_reply": "2023-05-25T11:53:40.996776Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2023-05-25 13:53:40 18352:18352 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-25 13:53:40 18352:18352 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-25 13:53:40 18352:18352 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "STAGE:2023-05-25 13:53:40 18352:18352 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-25 13:53:40 18352:18352 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-25 13:53:40 18352:18352 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "withgrad_latency=0.028768240939825773\n",
      "gradless_latency=0.021544719114899635\n",
      "withgrad_memory=487282176\n",
      "gradless_memory=512\n"
     ]
    }
   ],
   "source": [
    "gvp_model.train()\n",
    "with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "             profile_memory=True) as prof_train:\n",
    "    withgrad_latency, withgrad_memory = test_latency_memory(gvp_model, dataset_sample)\n",
    "    # out = gvp_model(dataset_sample)\n",
    "\n",
    "\n",
    "gvp_model.eval()\n",
    "with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "            profile_memory=True) as prof_test:\n",
    "    with torch.no_grad():\n",
    "        gradless_latency, gradless_memory = test_latency_memory(gvp_model, dataset_sample)\n",
    "        # out = gvp_model(dataset_sample)\n",
    "\n",
    "print(f\"{withgrad_latency=}\")\n",
    "print(f\"{gradless_latency=}\")\n",
    "print(f\"{withgrad_memory=}\")\n",
    "print(f\"{gradless_memory=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-25T11:53:41.002649Z",
     "iopub.status.busy": "2023-05-25T11:53:41.002379Z",
     "iopub.status.idle": "2023-05-25T11:53:41.152969Z",
     "shell.execute_reply": "2023-05-25T11:53:41.152401Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================================================================================================================================================================================================================================\n",
      "This report only display top-level ops statistics\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                           aten::linear         2.36%     446.000us        35.94%       6.803ms      62.413us       0.000us         0.00%       6.345ms      58.211us           0 b           0 b     185.26 Mb       1.23 Mb           109  \n",
      "                                   volta_sgemm_64x64_tt         0.00%       0.000us         0.00%       0.000us       0.000us       2.787ms        27.33%       2.787ms     253.364us           0 b           0 b           0 b           0 b            11  \n",
      "                                  volta_sgemm_128x64_tn         0.00%       0.000us         0.00%       0.000us       0.000us       2.065ms        20.25%       2.065ms     103.250us           0 b           0 b           0 b           0 b            20  \n",
      "                                  volta_sgemm_32x128_tn         0.00%       0.000us         0.00%       0.000us       0.000us       1.294ms        12.69%       1.294ms      28.130us           0 b           0 b           0 b           0 b            46  \n",
      "                                              aten::cat         3.52%     667.000us         4.99%     945.000us      22.500us       1.273ms        12.48%       1.273ms      30.310us           0 b           0 b     253.17 Mb     253.17 Mb            42  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us       1.126ms        11.04%       1.126ms      30.432us           0 b           0 b           0 b           0 b            37  \n",
      "void at::native::(anonymous namespace)::indexSelectL...         0.00%       0.000us         0.00%       0.000us       0.000us     385.000us         3.78%     385.000us      18.333us           0 b           0 b           0 b           0 b            21  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     330.000us         3.24%     330.000us       5.410us           0 b           0 b           0 b           0 b            61  \n",
      "                                              aten::sum         3.87%     732.000us         5.02%     951.000us      24.385us     324.000us         3.18%     324.000us       8.308us           0 b           0 b      18.36 Mb      18.36 Mb            39  \n",
      "                                              aten::mul         3.80%     720.000us         5.32%       1.006ms      21.870us     251.000us         2.46%     251.000us       5.457us           0 b           0 b      39.42 Mb      39.42 Mb            46  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 18.927ms\n",
      "Self CUDA time total: 10.197ms\n",
      "\n",
      "===========================================================================================================================================================================================================================================================\n",
      "This report only display top-level ops statistics\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                              aten::cat         3.52%     667.000us         4.99%     945.000us      22.500us       1.273ms        12.48%       1.273ms      30.310us           0 b           0 b     253.17 Mb     253.17 Mb            42  \n",
      "                                           aten::linear         2.36%     446.000us        35.94%       6.803ms      62.413us       0.000us         0.00%       6.345ms      58.211us           0 b           0 b     185.26 Mb       1.23 Mb           109  \n",
      "                                             aten::relu         0.60%     114.000us         1.97%     373.000us      23.312us       0.000us         0.00%     219.000us      13.688us           0 b           0 b      54.75 Mb           0 b            16  \n",
      "                                           aten::square         0.30%      57.000us         5.06%     957.000us      24.538us       0.000us         0.00%     204.000us       5.231us           0 b           0 b      52.90 Mb           0 b            39  \n",
      "                                              aten::mul         3.80%     720.000us         5.32%       1.006ms      21.870us     251.000us         2.46%     251.000us       5.457us           0 b           0 b      39.42 Mb      39.42 Mb            46  \n",
      "                                             aten::sqrt         2.56%     484.000us         3.67%     695.000us      17.821us      98.000us         0.96%      98.000us       2.513us           0 b           0 b      18.62 Mb      18.62 Mb            39  \n",
      "                                              aten::sum         3.87%     732.000us         5.02%     951.000us      24.385us     324.000us         3.18%     324.000us       8.308us           0 b           0 b      18.36 Mb      18.36 Mb            39  \n",
      "                                            aten::clamp         3.34%     632.000us         4.60%     871.000us      19.795us     109.000us         1.07%     109.000us       2.477us           0 b           0 b      17.93 Mb      17.93 Mb            44  \n",
      "                                          aten::sigmoid         2.27%     430.000us         3.09%     585.000us      22.500us      78.000us         0.76%      78.000us       3.000us           0 b           0 b      12.64 Mb      12.64 Mb            26  \n",
      "                                              aten::add         2.23%     423.000us         3.37%     638.000us      15.950us     108.000us         1.06%     108.000us       2.700us           0 b           0 b       6.23 Mb       6.23 Mb            40  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 18.927ms\n",
      "Self CUDA time total: 10.197ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prof_train.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10, top_level_events_only=True))\n",
    "print(prof_train.key_averages().table(sort_by=\"cuda_memory_usage\", row_limit=10, top_level_events_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-25T11:53:41.157872Z",
     "iopub.status.busy": "2023-05-25T11:53:41.157602Z",
     "iopub.status.idle": "2023-05-25T11:53:41.293583Z",
     "shell.execute_reply": "2023-05-25T11:53:41.292398Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================================================================================================================================================================================================================================\n",
      "This report only display top-level ops statistics\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                           aten::linear         4.81%     672.000us        37.11%       5.182ms      47.541us       0.000us         0.00%       6.199ms      56.872us           0 b           0 b     184.69 Mb      15.47 Mb           109  \n",
      "                                   volta_sgemm_64x64_tt         0.00%       0.000us         0.00%       0.000us       0.000us       2.787ms        27.87%       2.787ms     253.364us           0 b           0 b           0 b           0 b            11  \n",
      "                                  volta_sgemm_128x64_tn         0.00%       0.000us         0.00%       0.000us       0.000us       2.067ms        20.67%       2.067ms     103.350us           0 b           0 b           0 b           0 b            20  \n",
      "                                  volta_sgemm_32x128_tn         0.00%       0.000us         0.00%       0.000us       0.000us       1.302ms        13.02%       1.302ms      28.304us           0 b           0 b           0 b           0 b            46  \n",
      "                                              aten::cat         3.48%     486.000us         5.23%     731.000us      17.405us       1.269ms        12.69%       1.269ms      30.214us           0 b           0 b     252.66 Mb     252.66 Mb            42  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us       1.119ms        11.19%       1.119ms      30.243us           0 b           0 b           0 b           0 b            37  \n",
      "void at::native::(anonymous namespace)::indexSelectL...         0.00%       0.000us         0.00%       0.000us       0.000us     385.000us         3.85%     385.000us      18.333us           0 b           0 b           0 b           0 b            21  \n",
      "                                              aten::sum         4.17%     583.000us         5.63%     786.000us      20.154us     328.000us         3.28%     328.000us       8.410us           0 b           0 b      20.85 Mb      20.85 Mb            39  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     327.000us         3.27%     327.000us       5.361us           0 b           0 b           0 b           0 b            61  \n",
      "                                             aten::relu         0.39%      55.000us         2.00%     279.000us      17.438us       0.000us         0.00%     217.000us      13.562us           0 b           0 b      54.75 Mb           0 b            16  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 13.965ms\n",
      "Self CUDA time total: 10.001ms\n",
      "\n",
      "===========================================================================================================================================================================================================================================================\n",
      "This report only display top-level ops statistics\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                              aten::cat         3.48%     486.000us         5.23%     731.000us      17.405us       1.269ms        12.69%       1.269ms      30.214us           0 b           0 b     252.66 Mb     252.66 Mb            42  \n",
      "                                           aten::linear         4.81%     672.000us        37.11%       5.182ms      47.541us       0.000us         0.00%       6.199ms      56.872us           0 b           0 b     184.69 Mb      15.47 Mb           109  \n",
      "                                             aten::relu         0.39%      55.000us         2.00%     279.000us      17.438us       0.000us         0.00%     217.000us      13.562us           0 b           0 b      54.75 Mb           0 b            16  \n",
      "                                           aten::square         0.64%      90.000us         5.69%     794.000us      20.359us       0.000us         0.00%     182.000us       4.667us           0 b           0 b      52.61 Mb       5.08 Mb            39  \n",
      "                                              aten::mul         2.56%     357.000us         3.57%     498.000us      19.154us     196.000us         1.96%     196.000us       7.538us           0 b           0 b      38.25 Mb      38.25 Mb            26  \n",
      "                                              aten::sum         4.17%     583.000us         5.63%     786.000us      20.154us     328.000us         3.28%     328.000us       8.410us           0 b           0 b      20.85 Mb      20.85 Mb            39  \n",
      "                                             aten::sqrt         2.26%     315.000us         3.67%     512.000us      13.128us      98.000us         0.98%      98.000us       2.513us           0 b           0 b      20.51 Mb      20.51 Mb            39  \n",
      "                                            aten::clamp         3.29%     459.000us         4.88%     682.000us      15.500us     108.000us         1.08%     108.000us       2.455us           0 b           0 b      17.55 Mb      17.55 Mb            44  \n",
      "                                          aten::sigmoid         2.23%     312.000us         3.21%     448.000us      17.231us      73.000us         0.73%      73.000us       2.808us           0 b           0 b      12.64 Mb      12.64 Mb            26  \n",
      "                                              aten::add         2.79%     389.000us         4.42%     617.000us      15.425us     105.000us         1.05%     105.000us       2.625us           0 b           0 b       6.23 Mb       6.23 Mb            40  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 13.965ms\n",
      "Self CUDA time total: 10.001ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prof_test.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10, top_level_events_only=True))\n",
    "print(prof_test.key_averages().table(sort_by=\"cuda_memory_usage\", row_limit=10, top_level_events_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-25T11:53:41.299154Z",
     "iopub.status.busy": "2023-05-25T11:53:41.298625Z",
     "iopub.status.idle": "2023-05-25T11:53:41.994789Z",
     "shell.execute_reply": "2023-05-25T11:53:41.994255Z"
    }
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Here comes the code for the steerable implementation.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHere comes the code for the steerable implementation.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Here comes the code for the steerable implementation."
     ]
    }
   ],
   "source": [
    "raise NotImplementedError(\"Here comes the code for the steerable implementation.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "It seems that the profiling does not take into account the fact that gradients have to be stored.  We think this, because the additional memory allocated to CUDA is more than 400 MB when storing the gradients, but the profiler only gives 250 MB at most."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gvp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
