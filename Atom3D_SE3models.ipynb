{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ca0cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Download atom3dutils.py in same directory!\n",
    "import atom3dutils\n",
    "from atom3dutils import get_datasets, get_metrics\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchmetrics\n",
    "import torch_scatter\n",
    "import torch_geometric.data.batch as tg_batch\n",
    "from torch_geometric.nn import MessagePassing, global_mean_pool\n",
    "from torch_geometric.transforms import RadiusGraph, Compose, BaseTransform, Distance, Cartesian, RandomRotate\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch_geometric as tg\n",
    "\n",
    "from e3nn import o3\n",
    "from e3nn.o3 import Irreps\n",
    "from e3nn.nn import Gate\n",
    "\n",
    "import lightning.pytorch as lp\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "lp.seed_everything(42, workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f435d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balanced_irreps(hidden_features, lmax):\n",
    "    \"\"\"Divide subspaces equally over the feature budget\"\"\"\n",
    "    N = int(hidden_features / (lmax + 1))\n",
    "\n",
    "    irreps = []\n",
    "    for l, irrep in enumerate(Irreps.spherical_harmonics(lmax)):\n",
    "        n = int(N / (2 * l + 1))\n",
    "\n",
    "        irreps.append(str(n) + \"x\" + str(irrep[1]))\n",
    "\n",
    "    irreps = \"+\".join(irreps)\n",
    "\n",
    "    irreps = Irreps(irreps)\n",
    "\n",
    "    # Don't short sell yourself, add some more trivial irreps to fill the gap\n",
    "    gap = hidden_features - irreps.dim\n",
    "    if gap > 0:\n",
    "        irreps = Irreps(\"{}x0e\".format(gap)) + irreps\n",
    "        irreps = irreps.simplify()\n",
    "\n",
    "    return irreps\n",
    "\n",
    "def compute_gate_irreps(irreps_out):\n",
    "    \"\"\"Compute irreps_scalars, irreps\"\"\"\n",
    "    irreps_scalars = Irreps([(mul, ir) for mul, ir in irreps_out if ir.l == 0])\n",
    "    irreps_gated = Irreps([(mul, ir) for mul, ir in irreps_out if ir.l > 0])\n",
    "    irreps_gates = Irreps([(mul, \"0e\") for mul, _ in irreps_gated]).simplify()\n",
    "\n",
    "    return irreps_scalars, irreps_gated, irreps_gates\n",
    "\n",
    "class Convolution(nn.Module):\n",
    "    \"\"\" SE(3) equivariant convolution, parameterised by a radial network \"\"\"\n",
    "    def __init__(self, irreps_in1, irreps_in2, irreps_out):\n",
    "        super().__init__()\n",
    "        self.irreps_in1 = irreps_in1\n",
    "        self.irreps_in2 = irreps_in2\n",
    "        self.irreps_out = irreps_out\n",
    "        self.tp =  o3.FullyConnectedTensorProduct(\n",
    "            irreps_in1,\n",
    "            irreps_in2,\n",
    "            irreps_out,\n",
    "            irrep_normalization=\"component\",\n",
    "            path_normalization=\"element\",\n",
    "            internal_weights=False,\n",
    "            shared_weights=False\n",
    "        )\n",
    "\n",
    "        self.radial_net = RadialNet(self.tp.weight_numel)\n",
    "\n",
    "    def forward(self, x, rel_pos_sh, distance):\n",
    "        \"\"\"\n",
    "        Features of shape [E, irreps_in1.dim]\n",
    "        rel_pos_sh of shape [E, irreps_in2.dim]\n",
    "        distance of shape [E, 1]\n",
    "        \"\"\"\n",
    "        weights = self.radial_net(distance)\n",
    "        return self.tp(x, rel_pos_sh, weights)\n",
    "\n",
    "class RadialNet(nn.Module):\n",
    "    def __init__(self, num_weights):\n",
    "        super().__init__()\n",
    "\n",
    "        num_basis = 10\n",
    "        basis = tg.nn.models.dimenet.BesselBasisLayer(num_basis, cutoff=4)\n",
    "\n",
    "        self.net = nn.Sequential(basis,\n",
    "                                nn.Linear(num_basis, 16),\n",
    "                                nn.SiLU(),\n",
    "                                nn.Linear(16, num_weights))\n",
    "    def forward(self, dist):\n",
    "        return self.net(dist.squeeze(-1))\n",
    "\n",
    "\n",
    "class ConvLayerSE3(tg.nn.MessagePassing):\n",
    "    def __init__(self, irreps_in1, irreps_in2, irreps_out, activation=True):\n",
    "        super().__init__(aggr=\"add\")\n",
    "\n",
    "        self.irreps_in1 = irreps_in1\n",
    "        self.irreps_in2 = irreps_in2\n",
    "        self.irreps_out = irreps_out\n",
    "\n",
    "        irreps_scalars, irreps_gated, irreps_gates = compute_gate_irreps(irreps_out)\n",
    "        self.conv = Convolution(irreps_in1, irreps_in2, irreps_gates + irreps_out)\n",
    "\n",
    "        if activation:\n",
    "            self.gate = Gate(irreps_scalars, [nn.SiLU()], irreps_gates, [nn.Sigmoid()], irreps_gated)\n",
    "        else:\n",
    "            self.gate = nn.Identity()\n",
    "\n",
    "    def forward(self, edge_index, x, rel_pos_sh, dist):\n",
    "        x = self.propagate(edge_index, x=x, rel_pos_sh=rel_pos_sh, dist=dist)\n",
    "        x = self.gate(x)\n",
    "        return x\n",
    "\n",
    "    def message(self, x_j, rel_pos_sh, dist):\n",
    "        return self.conv(x_j, rel_pos_sh, dist)\n",
    "\n",
    "class ConvModel(nn.Module):\n",
    "    def __init__(self, irreps_in, irreps_hidden, irreps_edge, irreps_out, depth, max_z:int=atom3dutils._NUM_ATOM_TYPES):\n",
    "        super().__init__()\n",
    "\n",
    "        self.irreps_in = irreps_in\n",
    "        self.irreps_hidden = irreps_hidden\n",
    "        self.irreps_edge = irreps_edge\n",
    "        self.irreps_out = irreps_out\n",
    "\n",
    "        self.embedder = nn.Embedding(max_z, irreps_in.dim)\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(ConvLayerSE3(irreps_in, irreps_edge, irreps_hidden))\n",
    "        for i in range(depth-2):\n",
    "            self.layers.append(ConvLayerSE3(irreps_hidden, irreps_edge, irreps_hidden))\n",
    "        self.layers.append(ConvLayerSE3(irreps_hidden, irreps_edge, irreps_out, activation=False))\n",
    "\n",
    "\n",
    "    def forward(self, graph):\n",
    "        edge_index = graph.edge_index\n",
    "        z = graph.z\n",
    "        pos = graph.pos\n",
    "        batch = graph.batch\n",
    "\n",
    "        # Prepare quantities for convolutional layers\n",
    "        src, tgt = edge_index[0], edge_index[1]\n",
    "        rel_pos = pos[tgt] - pos[src]\n",
    "        rel_pos_sh = o3.spherical_harmonics(self.irreps_edge, rel_pos, normalize=True)\n",
    "        dist = torch.linalg.vector_norm(rel_pos, dim=-1, keepdims=True)\n",
    "\n",
    "        x = self.embedder(z)\n",
    "        # Let's go!\n",
    "        for layer in self.layers:\n",
    "            x = layer(edge_index, x, rel_pos_sh, dist)\n",
    "\n",
    "        # 1-dim output, squeeze it out\n",
    "        x = x.squeeze(-1)\n",
    "        \n",
    "        # Global pooling\n",
    "        x = tg.nn.global_mean_pool(x, batch)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9577fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Atom3D(lp.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model:nn.Module,\n",
    "        metrics:dict[str,torchmetrics.Metric],\n",
    "        lr:float=1e-4,\n",
    "        *args,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "\n",
    "        self.metrics = metrics\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "    def forward(self, batch:tg_batch):\n",
    "        return self.model(batch)\n",
    "\n",
    "    def training_step(self, batch:tg_batch, batch_idx:int):\n",
    "        out = self(batch)\n",
    "        loss = self.loss_fn(out, batch.label)\n",
    "                \n",
    "        self.log(\"train/loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        if trainer.optimizers:\n",
    "            self.log('train/lr',self.optimizers[0].param_groups[0]['lr'])\n",
    "\n",
    "    def validation_step(self, batch:tg_batch, batch_idx:int):\n",
    "        out = self(batch)\n",
    "        loss = self.loss_fn(out, batch.label)\n",
    "        \n",
    "        self.log(\"val/loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch:tg_batch, batch_idx:int):\n",
    "        out = self(batch)\n",
    "        \n",
    "        results = dict()\n",
    "        for key, func in self.metrics.items():\n",
    "            results[f'test/{key}'] = func(out, batch.label)\n",
    "        self.log_dict(results, on_epoch=True, logger=True)\n",
    "        \n",
    "        return self.loss_fn(out, batch.label)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        \n",
    "        return {\n",
    "            'optimizer': optimizer\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e451283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL\n",
    "num_features = 32      # <- can be anything, we must test the impact!\n",
    "l_max = 1              # <- has to be >0, increases model complexity REALLY fast\n",
    "depth = 3              # <- nr of layers, must be >1\n",
    "\n",
    "# TRAINING\n",
    "epochs = 1\n",
    "lr = 1e-3              # <- makes use of Adam, so doesnt really matter\n",
    "batch_size = 2         # <- Choose biggest that doesnt crash\n",
    "num_workers = 4        # <- just so we dont get a warning\n",
    "\n",
    "# DATA\n",
    "datadir='/home/jip/Desktop/tmp/' # <- TODO: change to whatever works for you\n",
    "task='LBA'             # <-'PPI','RSR','PSR','MSP','LEP','LBA','SMP'\n",
    "smp_idx=3              # <- range 0-19 (incl), only matters if task=='SMP'\n",
    "lba_split=30           # <- 30 or 60, only matters if task=='LBA'\n",
    "\n",
    "# LOGGING\n",
    "logdir='./runs/'       # <- tensorboard --logdir=...\n",
    "modeldir='./models/'   # <- saves top-2 and last models here\n",
    "\n",
    "# TESTING \n",
    "# if set -> dont train only test, \n",
    "# otherwise -> train first, then test best model\n",
    "test=None\n",
    "test='/home/jip/Desktop/tmp/lba_split30_batch4_epoch14-step13155.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e66b125",
   "metadata": {},
   "outputs": [],
   "source": [
    "irreps_in = (Irreps(\"1x0e\")*num_features).simplify()\n",
    "irreps_hidden = (Irreps.spherical_harmonics(l_max)*num_features).sort()[0].simplify()\n",
    "irreps_edge = Irreps.spherical_harmonics(l_max)\n",
    "irreps_out = Irreps(\"1x0e\")\n",
    "\n",
    "print(\"Input irreps\", irreps_in)\n",
    "print(\"Hidden irreps\", irreps_hidden)\n",
    "print(\"Edge irreps\", irreps_edge)\n",
    "print(\"Output irreps\", irreps_out)\n",
    "print(\"Dim hidden irreps:\", irreps_hidden.dim)\n",
    "\n",
    "model = ConvModel(irreps_in, irreps_hidden, irreps_edge, irreps_out, depth)\n",
    "print()\n",
    "print(model.embedder)\n",
    "print(model.layers[0].conv)\n",
    "print(model.layers[1].conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624ad290",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics:dict[str,callable]=get_metrics(task)\n",
    "print(\"Test metrics:\", list(metrics.keys()))\n",
    "\n",
    "datasets:dict[str,any] = get_datasets(\n",
    "    task=task, \n",
    "    smp_idx=smp_idx,\n",
    "    lba_split=lba_split,\n",
    "    data_dir=datadir)\n",
    "\n",
    "dataloaders:dict[str,tg.loader.DataLoader] = {\n",
    "    \"train\": tg.loader.DataLoader(datasets['train'], batch_size=batch_size, num_workers=num_workers, shuffle=True),\n",
    "    \"valid\": tg.loader.DataLoader(datasets['valid'], batch_size=batch_size, num_workers=num_workers),\n",
    "    \"test\":  tg.loader.DataLoader(datasets['test'],  batch_size=batch_size, num_workers=num_workers),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7893630",
   "metadata": {},
   "outputs": [],
   "source": [
    "_name = str(task)\n",
    "if task == 'SMP':\n",
    "    _name+=f'-smp_idx={smp_idx}'\n",
    "elif task == 'LBA':\n",
    "    _name+=f'-lba_split={lba_split}'\n",
    "\n",
    "_version = f\"{time.strftime('%Y%b%d-%T')}\"\n",
    "    \n",
    "logger = TensorBoardLogger(\n",
    "    save_dir=logdir, \n",
    "    name=_name,\n",
    "    version=_version, \n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=modeldir, \n",
    "    save_top_k=2, \n",
    "    monitor=\"val_loss\",\n",
    "    mode='min',\n",
    "    save_on_train_epoch_end=True,\n",
    "    filename=_name+\"-epoch={epoch:02d}-metric={val_metric:.2e}\",\n",
    "    save_last=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda517a4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plmodule = Atom3D(\n",
    "    model=model, \n",
    "    metrics=metrics,\n",
    "    lr=lr,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98274fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = lp.Trainer(\n",
    "    max_epochs=epochs,\n",
    "    logger=logger,\n",
    "    default_root_dir=modeldir,\n",
    "    callbacks=[checkpoint_callback,],\n",
    ")\n",
    "\n",
    "if not test:\n",
    "    trainer.fit(plmodule, dataloaders[\"train\"], dataloaders[\"valid\"])\n",
    "    print(\"Best model:\", checkpoint_callback.best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4752ff82",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if checkpoint_callback.best_model_path:\n",
    "    results = trainer.test(plmodule, dataloaders['test'], ckpt_path=checkpoint_callback.best_model_path)\n",
    "elif test:\n",
    "    results = trainer.test(plmodule, dataloaders['test'], ckpt_path=test)\n",
    "else:\n",
    "    print(\"Could not find a model to test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c37dcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
