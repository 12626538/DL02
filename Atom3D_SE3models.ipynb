{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67ca0cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Download atom3dutils.py in same directory!\n",
    "import atom3dutils\n",
    "from atom3dutils import get_datasets, get_metrics\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchmetrics\n",
    "import torch_scatter\n",
    "import torch_geometric.data.batch as tg_batch\n",
    "from torch_geometric.nn import MessagePassing, global_mean_pool\n",
    "from torch_geometric.transforms import RadiusGraph, Compose, BaseTransform, Distance, Cartesian, RandomRotate\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch_geometric as tg\n",
    "\n",
    "from e3nn import o3\n",
    "from e3nn.o3 import Irreps\n",
    "from e3nn.nn import Gate\n",
    "\n",
    "import lightning.pytorch as lp\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "lp.seed_everything(42, workers=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "51b3cd8c",
   "metadata": {},
   "source": [
    "# PSA\n",
    "This is copido from [here](https://colab.research.google.com/drive/1tTG0rJjLhHHnO7eXtMXmh0mRrGY95r0J#scrollTo=oKvMIMMr3gWg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f435d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balanced_irreps(hidden_features, lmax):\n",
    "    \"\"\"Divide subspaces equally over the feature budget\"\"\"\n",
    "    N = int(hidden_features / (lmax + 1))\n",
    "\n",
    "    irreps = []\n",
    "    for l, irrep in enumerate(Irreps.spherical_harmonics(lmax)):\n",
    "        n = int(N / (2 * l + 1))\n",
    "\n",
    "        irreps.append(str(n) + \"x\" + str(irrep[1]))\n",
    "\n",
    "    irreps = \"+\".join(irreps)\n",
    "\n",
    "    irreps = Irreps(irreps)\n",
    "\n",
    "    # Don't short sell yourself, add some more trivial irreps to fill the gap\n",
    "    gap = hidden_features - irreps.dim\n",
    "    if gap > 0:\n",
    "        irreps = Irreps(\"{}x0e\".format(gap)) + irreps\n",
    "        irreps = irreps.simplify()\n",
    "\n",
    "    return irreps\n",
    "\n",
    "def compute_gate_irreps(irreps_out):\n",
    "    \"\"\"Compute irreps_scalars, irreps\"\"\"\n",
    "    irreps_scalars = Irreps([(mul, ir) for mul, ir in irreps_out if ir.l == 0])\n",
    "    irreps_gated = Irreps([(mul, ir) for mul, ir in irreps_out if ir.l > 0])\n",
    "    irreps_gates = Irreps([(mul, \"0e\") for mul, _ in irreps_gated]).simplify()\n",
    "\n",
    "    return irreps_scalars, irreps_gated, irreps_gates\n",
    "\n",
    "class Convolution(nn.Module):\n",
    "    \"\"\" SE(3) equivariant convolution, parameterised by a radial network \"\"\"\n",
    "    def __init__(self, irreps_in1, irreps_in2, irreps_out):\n",
    "        super().__init__()\n",
    "        self.irreps_in1 = irreps_in1\n",
    "        self.irreps_in2 = irreps_in2\n",
    "        self.irreps_out = irreps_out\n",
    "        self.tp =  o3.FullyConnectedTensorProduct(\n",
    "            irreps_in1,\n",
    "            irreps_in2,\n",
    "            irreps_out,\n",
    "            irrep_normalization=\"component\",\n",
    "            path_normalization=\"element\",\n",
    "            internal_weights=False,\n",
    "            shared_weights=False\n",
    "        )\n",
    "\n",
    "        self.radial_net = RadialNet(self.tp.weight_numel)\n",
    "\n",
    "    def forward(self, x, rel_pos_sh, distance):\n",
    "        \"\"\"\n",
    "        Features of shape [E, irreps_in1.dim]\n",
    "        rel_pos_sh of shape [E, irreps_in2.dim]\n",
    "        distance of shape [E, 1]\n",
    "        \"\"\"\n",
    "        weights = self.radial_net(distance)\n",
    "        return self.tp(x, rel_pos_sh, weights)\n",
    "\n",
    "class RadialNet(nn.Module):\n",
    "    def __init__(self, num_weights):\n",
    "        super().__init__()\n",
    "\n",
    "        num_basis = 10\n",
    "        basis = tg.nn.models.dimenet.BesselBasisLayer(num_basis, cutoff=4)\n",
    "\n",
    "        self.net = nn.Sequential(basis,\n",
    "                                nn.Linear(num_basis, 16),\n",
    "                                nn.SiLU(),\n",
    "                                nn.Linear(16, num_weights))\n",
    "    def forward(self, dist):\n",
    "        return self.net(dist.squeeze(-1))\n",
    "\n",
    "\n",
    "class ConvLayerSE3(tg.nn.MessagePassing):\n",
    "    def __init__(self, irreps_in1, irreps_in2, irreps_out, activation=True):\n",
    "        super().__init__(aggr=\"add\")\n",
    "\n",
    "        self.irreps_in1 = irreps_in1\n",
    "        self.irreps_in2 = irreps_in2\n",
    "        self.irreps_out = irreps_out\n",
    "\n",
    "        irreps_scalars, irreps_gated, irreps_gates = compute_gate_irreps(irreps_out)\n",
    "        self.conv = Convolution(irreps_in1, irreps_in2, irreps_gates + irreps_out)\n",
    "\n",
    "        if activation:\n",
    "            self.gate = Gate(irreps_scalars, [nn.SiLU()], irreps_gates, [nn.Sigmoid()], irreps_gated)\n",
    "        else:\n",
    "            self.gate = nn.Identity()\n",
    "\n",
    "    def forward(self, edge_index, x, rel_pos_sh, dist):\n",
    "        x = self.propagate(edge_index, x=x, rel_pos_sh=rel_pos_sh, dist=dist)\n",
    "        x = self.gate(x)\n",
    "        return x\n",
    "\n",
    "    def message(self, x_i, x_j, rel_pos_sh, dist):\n",
    "        print(type(x_i))\n",
    "        return self.conv(x_j, rel_pos_sh, dist)\n",
    "\n",
    "class ConvModel(nn.Module):\n",
    "    def __init__(self, irreps_in, irreps_hidden, irreps_edge, irreps_out, depth, max_z:int=atom3dutils._NUM_ATOM_TYPES):\n",
    "        super().__init__()\n",
    "\n",
    "        self.irreps_in = irreps_in\n",
    "        self.irreps_hidden = irreps_hidden\n",
    "        self.irreps_edge = irreps_edge\n",
    "        self.irreps_out = irreps_out\n",
    "\n",
    "        self.embedder = nn.Embedding(max_z, irreps_in.dim)\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(ConvLayerSE3(irreps_in, irreps_edge, irreps_hidden))\n",
    "        for i in range(depth-2):\n",
    "            self.layers.append(ConvLayerSE3(irreps_hidden, irreps_edge, irreps_hidden))\n",
    "        self.layers.append(ConvLayerSE3(irreps_hidden, irreps_edge, irreps_out, activation=False))\n",
    "\n",
    "    def forward(self, graph):\n",
    "        edge_index = graph.edge_index\n",
    "        z = graph.z\n",
    "        pos = graph.pos\n",
    "        batch = graph.batch\n",
    "\n",
    "        # Prepare quantities for convolutional layers\n",
    "        # Index of source and target node\n",
    "        src, tgt = edge_index[0], edge_index[1]\n",
    "        # Vector pointing from the source node to the target node\n",
    "        rel_pos = pos[tgt] - pos[src]\n",
    "        # That vector in Spherical Harmonics\n",
    "        rel_pos_sh = o3.spherical_harmonics(self.irreps_edge, rel_pos, normalize=True)\n",
    "        # The norm of that vector\n",
    "        dist = torch.linalg.vector_norm(rel_pos, dim=-1, keepdims=True)\n",
    "\n",
    "        # Embed atom one-hot\n",
    "        x = self.embedder(z)\n",
    "\n",
    "        # Convolve nodes\n",
    "        for layer in self.layers:\n",
    "            x = layer(edge_index, x, rel_pos_sh, dist)\n",
    "\n",
    "        # 1-dim output, squeeze it out\n",
    "        x = x.squeeze(-1)\n",
    "\n",
    "        # TODO: add dense layers\n",
    "        \n",
    "        # Global pooling of node features\n",
    "        x = tg.nn.global_mean_pool(x, batch)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e9577fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Atom3D(lp.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model:nn.Module,\n",
    "        metrics:dict[str,torchmetrics.Metric],\n",
    "        lr:float=1e-4,\n",
    "        *args,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "\n",
    "        self.metrics = metrics\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "    def forward(self, batch:tg_batch):\n",
    "        return self.model(batch)\n",
    "\n",
    "    def training_step(self, batch:tg_batch, batch_idx:int):\n",
    "        out = self(batch)\n",
    "        loss = self.loss_fn(out, batch.label)\n",
    "                \n",
    "        self.log(\"train/loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch:tg_batch, batch_idx:int):\n",
    "        out = self(batch)\n",
    "        loss = self.loss_fn(out, batch.label)\n",
    "        \n",
    "        self.log(\"val/loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch:tg_batch, batch_idx:int):\n",
    "        out = self(batch)\n",
    "\n",
    "        out = out.detach().cpu()\n",
    "        label = batch.label.detach().cpu()\n",
    "\n",
    "        results = dict()\n",
    "        for key, func in self.metrics.items():\n",
    "            results[f'test/{key}'] = func(out, label)\n",
    "        self.log_dict(results, on_epoch=True, logger=True)\n",
    "        \n",
    "        return self.loss_fn(out, label)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        \n",
    "        return {\n",
    "            'optimizer': optimizer\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e451283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL\n",
    "num_features = 16      # <- can be anything, we must test the impact!\n",
    "l_max = 1              # <- has to be >0, increases model complexity REALLY fast\n",
    "depth = 3              # <- nr of layers, must be >1\n",
    "\n",
    "# TRAINING\n",
    "epochs = 1\n",
    "lr = 1e-3              # <- makes use of Adam, so doesnt really matter\n",
    "batch_size = 2         # <- Choose biggest that doesnt crash\n",
    "num_workers = 4        # <- just so we dont get a warning\n",
    "\n",
    "# DATA\n",
    "datadir='/media/jip/T7/DL02/data/' # <- TODO: change to whatever works for you\n",
    "task='LBA'             # <-'PPI','RSR','PSR','MSP','LEP','LBA','SMP'\n",
    "smp_idx=3              # <- range 0-19 (incl), only matters if task=='SMP'\n",
    "lba_split=60           # <- 30 or 60, only matters if task=='LBA'\n",
    "\n",
    "# LOGGING\n",
    "logdir='./runs/'       # <- tensorboard --logdir=...\n",
    "modeldir='./models/'   # <- saves top-2 and last models here\n",
    "\n",
    "# TESTING \n",
    "# if set -> dont train only test, \n",
    "# otherwise -> train first, then test best model\n",
    "test=None\n",
    "# test='/home/jip/Desktop/DL02/repo/models/LBA-lba_split60-epochepoch28-metricval_metric0.00e00.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e66b125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input irreps 16x0e\n",
      "Hidden irreps 16x0e+16x1o+16x2e\n",
      "Edge irreps 1x1o\n",
      "Output irreps 1x0e\n",
      "Dim hidden irreps: 144\n",
      "\n",
      "Embedding(9, 16)\n",
      "Convolution(\n",
      "  (tp): FullyConnectedTensorProduct(16x0e x 1x1o -> 48x0e+16x1o+16x2e | 256 paths | 256 weights)\n",
      "  (radial_net): RadialNet(\n",
      "    (net): Sequential(\n",
      "      (0): BesselBasisLayer(\n",
      "        (envelope): Envelope()\n",
      "      )\n",
      "      (1): Linear(in_features=10, out_features=16, bias=True)\n",
      "      (2): SiLU()\n",
      "      (3): Linear(in_features=16, out_features=256, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Convolution(\n",
      "  (tp): FullyConnectedTensorProduct(16x0e+16x1o+16x2e x 1x1o -> 1x0e | 16 paths | 16 weights)\n",
      "  (radial_net): RadialNet(\n",
      "    (net): Sequential(\n",
      "      (0): BesselBasisLayer(\n",
      "        (envelope): Envelope()\n",
      "      )\n",
      "      (1): Linear(in_features=10, out_features=16, bias=True)\n",
      "      (2): SiLU()\n",
      "      (3): Linear(in_features=16, out_features=16, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "irreps_in = (Irreps(\"1x0e\")*num_features).simplify()\n",
    "irreps_hidden = (Irreps.spherical_harmonics(l_max)*num_features).sort()[0].simplify()\n",
    "irreps_edge = Irreps(\"1x1o\")\n",
    "irreps_out = Irreps(\"1x0e\")\n",
    "\n",
    "print(\"Input irreps\", irreps_in)\n",
    "print(\"Hidden irreps\", irreps_hidden)\n",
    "print(\"Edge irreps\", irreps_edge)\n",
    "print(\"Output irreps\", irreps_out)\n",
    "print(\"Dim hidden irreps:\", irreps_hidden.dim)\n",
    "\n",
    "model = ConvModel(irreps_in, irreps_hidden, irreps_edge, irreps_out, depth)\n",
    "print()\n",
    "print(model.embedder)\n",
    "print(model.layers[0].conv)\n",
    "print(model.layers[-1].conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "624ad290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test metrics: ['pearson', 'kendall', 'spearman', 'rmse']\n"
     ]
    }
   ],
   "source": [
    "metrics:dict[str,callable]=get_metrics(task)\n",
    "print(\"Test metrics:\", list(metrics.keys()))\n",
    "\n",
    "datasets:dict[str,any] = get_datasets(\n",
    "    task=task, \n",
    "    smp_idx=smp_idx,\n",
    "    lba_split=lba_split,\n",
    "    data_dir=datadir)\n",
    "\n",
    "dataloaders:dict[str,tg.loader.DataLoader] = {\n",
    "    \"train\": tg.loader.DataLoader(datasets['train'], batch_size=batch_size, num_workers=num_workers, shuffle=True),\n",
    "    \"valid\": tg.loader.DataLoader(datasets['valid'], batch_size=batch_size, num_workers=num_workers),\n",
    "    \"test\":  tg.loader.DataLoader(datasets['test'],  batch_size=batch_size, num_workers=num_workers),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7893630",
   "metadata": {},
   "outputs": [],
   "source": [
    "_name = str(task)\n",
    "if task == 'SMP':\n",
    "    _name+=f'-smp_idx={smp_idx}'\n",
    "elif task == 'LBA':\n",
    "    _name+=f'-lba_split={lba_split}'\n",
    "\n",
    "_version = f\"{time.strftime('%Y%b%d-%T')}\"\n",
    "    \n",
    "logger = TensorBoardLogger(\n",
    "    save_dir=logdir, \n",
    "    name=_name,\n",
    "    version=_version, \n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=modeldir, \n",
    "    save_top_k=2, \n",
    "    monitor=\"val_loss\",\n",
    "    mode='min',\n",
    "    save_on_train_epoch_end=True,\n",
    "    filename=_name+\"-{epoch:02d}-{val_loss:.2e}\",\n",
    "    save_last=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dda517a4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plmodule = Atom3D(\n",
    "    model=model, \n",
    "    metrics=metrics,\n",
    "    lr=lr,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b98274fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type      | Params\n",
      "--------------------------------------\n",
      "0 | model   | ConvModel | 31.4 K\n",
      "1 | loss_fn | MSELoss   | 0     \n",
      "--------------------------------------\n",
      "31.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "31.4 K    Total params\n",
      "0.126     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1782/1782 [03:01<00:00,  9.82it/s, v_num=2:10, train/loss_step=10.10, val/loss_step=16.80, val/loss_epoch=45.10, train/loss_epoch=45.90]"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'method' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[39m=\u001b[39m lp\u001b[39m.\u001b[39mTrainer(\n\u001b[1;32m      2\u001b[0m     max_epochs\u001b[39m=\u001b[39mepochs,\n\u001b[1;32m      3\u001b[0m     logger\u001b[39m=\u001b[39mlogger,\n\u001b[1;32m      4\u001b[0m     default_root_dir\u001b[39m=\u001b[39mmodeldir,\n\u001b[1;32m      5\u001b[0m     callbacks\u001b[39m=\u001b[39m[checkpoint_callback,],\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      8\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m test:\n\u001b[0;32m----> 9\u001b[0m     trainer\u001b[39m.\u001b[39;49mfit(plmodule, dataloaders[\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m], dataloaders[\u001b[39m\"\u001b[39;49m\u001b[39mvalid\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m     10\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBest model:\u001b[39m\u001b[39m\"\u001b[39m, checkpoint_callback\u001b[39m.\u001b[39mbest_model_path)\n",
      "File \u001b[0;32m~/miniconda3/envs/gvp/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:520\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    518\u001b[0m model \u001b[39m=\u001b[39m _maybe_unwrap_optimized(model)\n\u001b[1;32m    519\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_module \u001b[39m=\u001b[39m model\n\u001b[0;32m--> 520\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    521\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    522\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/gvp/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     43\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 44\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     46\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/miniconda3/envs/gvp/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:559\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_connector\u001b[39m.\u001b[39mattach_data(\n\u001b[1;32m    550\u001b[0m     model, train_dataloaders\u001b[39m=\u001b[39mtrain_dataloaders, val_dataloaders\u001b[39m=\u001b[39mval_dataloaders, datamodule\u001b[39m=\u001b[39mdatamodule\n\u001b[1;32m    551\u001b[0m )\n\u001b[1;32m    553\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    554\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[1;32m    555\u001b[0m     ckpt_path,\n\u001b[1;32m    556\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    557\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    558\u001b[0m )\n\u001b[0;32m--> 559\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[1;32m    561\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    562\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/gvp/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:935\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_signal_connector\u001b[39m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    932\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    933\u001b[0m \u001b[39m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    934\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 935\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m    937\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    938\u001b[0m \u001b[39m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    940\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/gvp/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:978\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    976\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_sanity_check()\n\u001b[1;32m    977\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[0;32m--> 978\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m    979\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    980\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnexpected state \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/gvp/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:202\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start()\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madvance()\n\u001b[0;32m--> 202\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mon_advance_end()\n\u001b[1;32m    203\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/gvp/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:368\u001b[0m, in \u001b[0;36m_FitLoop.on_advance_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[39m# call train epoch end hooks\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[39m# we always call callback hooks first, but here we need to make an exception for the callbacks that\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[39m# monitor a metric, otherwise they wouldn't be able to monitor a key logged in\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[39m# `LightningModule.on_train_epoch_end`\u001b[39;00m\n\u001b[1;32m    367\u001b[0m call\u001b[39m.\u001b[39m_call_callback_hooks(trainer, \u001b[39m\"\u001b[39m\u001b[39mon_train_epoch_end\u001b[39m\u001b[39m\"\u001b[39m, monitoring_callbacks\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m--> 368\u001b[0m call\u001b[39m.\u001b[39;49m_call_lightning_module_hook(trainer, \u001b[39m\"\u001b[39;49m\u001b[39mon_train_epoch_end\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    369\u001b[0m call\u001b[39m.\u001b[39m_call_callback_hooks(trainer, \u001b[39m\"\u001b[39m\u001b[39mon_train_epoch_end\u001b[39m\u001b[39m\"\u001b[39m, monitoring_callbacks\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    371\u001b[0m trainer\u001b[39m.\u001b[39m_logger_connector\u001b[39m.\u001b[39mon_epoch_end()\n",
      "File \u001b[0;32m~/miniconda3/envs/gvp/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:142\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m hook_name\n\u001b[1;32m    141\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[LightningModule]\u001b[39m\u001b[39m{\u001b[39;00mpl_module\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    144\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    145\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "Cell \u001b[0;32mIn[4], line 31\u001b[0m, in \u001b[0;36mAtom3D.on_train_epoch_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mon_train_epoch_end\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     30\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizers:\n\u001b[0;32m---> 31\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog(\u001b[39m'\u001b[39m\u001b[39mtrain/lr\u001b[39m\u001b[39m'\u001b[39m,\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizers[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39mparam_groups[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m'\u001b[39m])\n",
      "\u001b[0;31mTypeError\u001b[0m: 'method' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "trainer = lp.Trainer(\n",
    "    max_epochs=epochs,\n",
    "    logger=logger,\n",
    "    default_root_dir=modeldir,\n",
    "    callbacks=[checkpoint_callback,],\n",
    ")\n",
    "\n",
    "if not test:\n",
    "    trainer.fit(plmodule, dataloaders[\"train\"], dataloaders[\"valid\"])\n",
    "    print(\"Best model:\", checkpoint_callback.best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4752ff82",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /home/jip/Desktop/DL02/repo/models/LBA-lba_split60-epochepoch28-metricval_metric0.00e00.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at /home/jip/Desktop/DL02/repo/models/LBA-lba_split60-epochepoch28-metricval_metric0.00e00.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:  97%|█████████▋| 220/226 [00:12<00:00, 17.08it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jip/miniconda3/envs/gvp/lib/python3.11/site-packages/scipy/stats/_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n",
      "/home/jip/miniconda3/envs/gvp/lib/python3.11/site-packages/scipy/stats/_stats_py.py:4916: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(warn_msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 226/226 [00:13<00:00, 17.06it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">   Runningstage.testing    </span>┃<span style=\"font-weight: bold\">                           </span>┃\n",
       "┃<span style=\"font-weight: bold\">          metric           </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test/kendall        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            nan            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test/pearson        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            nan            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test/rmse         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.2514996528625488     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test/spearman       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            nan            </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m  Runningstage.testing   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m                           \u001b[0m┃\n",
       "┃\u001b[1m \u001b[0m\u001b[1m         metric          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test/kendall       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           nan           \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test/pearson       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           nan           \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test/rmse        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.2514996528625488    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test/spearman      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           nan           \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if checkpoint_callback.best_model_path:\n",
    "    results = trainer.test(plmodule, dataloaders['test'], ckpt_path=checkpoint_callback.best_model_path)\n",
    "elif test:\n",
    "    results = trainer.test(plmodule, dataloaders['test'], ckpt_path=test)\n",
    "else:\n",
    "    print(\"Could not find a model to test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c37dcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
